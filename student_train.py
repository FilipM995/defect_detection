import os
import argparse
import json
import yaml

from eval import compute_metrics
from train import get_optimizer
from dataloaders import load_ksdd2, set_loader_params
from student import create_models
from models import create_models_from_config

import tensorflow as tf
from tensorflow import keras
from tensorflow.python.keras.utils.layer_utils import count_params
from time import time

def cli():
    parser = argparse.ArgumentParser(description='Bonseyes Defect Detection train script')
    parser.add_argument(
        '--input-channels', required=True, type=int, choices=[1, 3],
        help='Number of channels in the input images'
    )
    parser.add_argument(
        '--teacher-model-path', required=True, type=str,
        help='Path to Teacher model'
    )
    parser.add_argument(
        '--base-path', required=True, type=str,
        help='Path to Datatool output directory'
    )
    parser.add_argument(
        '--dataset-json-path', required=True, type=str,
        help='Path to dataset.json file generated by the Datatool'
    )
    parser.add_argument(
        '--train-percentage', required=True, type=float,
        help='Percentage of the dataset used for training'
    )
    parser.add_argument(
        '--height', required=True, type=int,
        help='Target height for input image resizing'
    )
    parser.add_argument(
        '--width', required=True, type=int,
        help='Target width for input image resizing'
    )
    parser.add_argument(
        '--dil-ksize', required=True, type=int,
        help='Kernel size for dilation of segmentation masks when calculating spatial weights for segmentation loss'
    )
    parser.add_argument(
        '--mixed-sup-N', required=True, type=int,
        help='Mixed supervision parameter - how many defective masks will be used for training'
    )
    parser.add_argument(
        '--dist-trans-w', required=True, type=float,
        help='Spatial weighting for the segmentation loss, formula w*x^p is applied to the output of the distance transform of segmentation masks'
    )
    parser.add_argument(
        '--dist-trans-p', required=True, type=float,
        help='Spatial weighting for the segmentation loss, formula w*x^p is applied to the output of the distance transform of segmentation masks'
    )
    parser.add_argument(
        '--shuffle-buf-size', required=True, type=int,
        help='Buffer size for dataset shuffling'
    )
    parser.add_argument(
        '--batch-size', required=True, type=int,
        help='Batch size'
    )
    parser.add_argument(
        '--optimizer', required=False, type=str,
        help='Optimizer to use for training'
    )
    parser.add_argument(
        '--learning-rate', required=True, type=float,
        help='Learning rate'
    )
    parser.add_argument(
        '--learning-decay', required=False, default=False, type=bool,
        help='Whether to use learning rate decay'
    )
    parser.add_argument(
        '--epochs', required=True, type=int,
        help='Number of epochs for training'
    )
    parser.add_argument(
        '--delta', required=True, type=float,
        help='Classification loss weight, relative to segmentation loss'
    )
    parser.add_argument(
        '--log-interval', required=True, type=int,
        help='Epoch interval for evaluating model performance during training and printing values of metrics'
    )
    parser.add_argument(
        '--alpha', required=True, type=float,
        help='Alpha parameter used in distillation loss'
    )
    parser.add_argument(
        '--temperature', required=True, type=int,
        help='Temperature parameter used in distillation loss'
    )
    parser.add_argument(
        '--metrics', required=False, nargs='*', type=str, default=[],
        help='List of metrics for model evaluation'
    )
    parser.add_argument(
        '--test-on-cpu', required=False, type=bool, default=False,
        help="Whether to test the model's performance on CPU after training"
    )
    parser.add_argument(
        '--tl-num-images', required=False, type=int,
        help='Number of images to use for transfer learning, if specified'
    )
    parser.add_argument(
        '--tl-pretrained-model', required=False, type=str,
        help='Path to a pretrained model when using transfer learning'
    )
    parser.add_argument(
        '--output-path', required=False, type=str,
        help='Path to a directory where the trained model will be saved'
    )
    parser.add_argument(
        '--networks-config-path', required=False, type=str, default='',
        help='Path to a config file specifying network architectures'
    )
    return parser

class MsModel():
    def __init__(self, seg_model, clf_model):
        self.seg_model = seg_model
        self.clf_model = clf_model

class Distiller():
    def __init__(self, student, teacher, optimizer, epochs, delta, log_interval, train_pos, train_neg_iter, test_set, test_on_cpu, distillation_loss_fn=keras.losses.KLDivergence(), alpha=0.1, temperature=3):
        self.teacher = teacher
        self.student = student
        self.optimizer = optimizer
        self.distillation_loss_fn = distillation_loss_fn
        self.alpha = alpha
        self.epochs = epochs
        self.temperature = temperature
        self.delta = delta
        self.log_interval = log_interval
        self.train_pos = train_pos
        self.train_neg_iter = train_neg_iter
        self.test_set = test_set
        self.test_on_cpu = test_on_cpu

    @tf.function(experimental_relax_shapes=True)
    def train_step(self, img, mask, segw, lbl, gamma, delta, Lambda):

        teacher_sf, teacher_sh = self.teacher.seg_model(img)
        teacher_cp = self.teacher.clf_model(tf.concat([teacher_sf, teacher_sh], axis=3))

        with tf.GradientTape() as seg_tape, tf.GradientTape() as clf_tape:
            student_sf, student_sh = self.student.seg_model(img)
            student_cp = self.student.clf_model(tf.concat([student_sf, student_sh], axis=3))
            Lseg = tf.reduce_sum(
                segw * tf.nn.sigmoid_cross_entropy_with_logits(logits=student_sh, labels=mask),
                axis=[1, 2]
            )
            Lclf = tf.nn.sigmoid_cross_entropy_with_logits(logits=student_cp, labels=lbl)

            seg_loss, clf_loss = tf.reduce_mean(Lambda*gamma*Lseg), tf.reduce_mean((1 - Lambda)*delta*Lclf)

            clf_distillation = self.distillation_loss_fn(
                tf.nn.softmax(teacher_cp / self.temperature, axis=1),
                tf.nn.softmax(student_cp / self.temperature, axis=1),
            )

            seg_distillation = self.distillation_loss_fn(
                tf.nn.softmax(teacher_sh / self.temperature, axis=1),
                tf.nn.softmax(student_sh / self.temperature, axis=1)
            )

            seg_loss = self.alpha * seg_loss + (1 - self.alpha) * seg_distillation

            clf_loss = self.alpha * clf_loss + (1 - self.alpha) * clf_distillation

        seg_grad = seg_tape.gradient(seg_loss, self.student.seg_model.trainable_variables)
        clf_grad = clf_tape.gradient(clf_loss, self.student.clf_model.trainable_variables)

        self.optimizer.apply_gradients(zip(seg_grad, self.student.seg_model.trainable_variables))
        self.optimizer.apply_gradients(zip(clf_grad, self.student.clf_model.trainable_variables))

    def train(self):
        for epoch in range(1, self.epochs + 1):
            start_time = time()
            Lambda = 1.0 - epoch/self.epochs
            for img, mask, segw, lbl, gamma in self.train_pos:
                self.train_step(img, mask, segw, lbl, gamma, self.delta, Lambda)
                _img, _mask, _segw, _lbl, _gamma = self.train_neg_iter.get_next()
                self.train_step(_img, _mask, _segw, _lbl, _gamma, self.delta, Lambda)
            end_time = time()
            print(f'Epoch: {epoch}, Training time: {end_time - start_time}')
            if epoch % self.log_interval == 0:
                metrics = compute_metrics(self.test_set, self.student.seg_model, self.student.clf_model)
                print(f'metrics: {metrics}')
            print('-' * 50)
        with tf.device("gpu:0"):
            start_time = time()
            metrics = compute_metrics(self.test_set, self.student.seg_model, self.student.clf_model)
            end_time = time()
            print(f'GPU test: {end_time - start_time}s')
        return metrics


def distill(teacher, student, epochs, delta, log_interval, train_pos, train_neg_iter, test, lr, test_on_cpu):

    distiller = Distiller(student=student, teacher=teacher, optimizer=get_optimizer(lr), 
        train_pos=train_pos, train_neg_iter=train_neg_iter, test_set=test, epochs=epochs, delta=delta, log_interval=log_interval, test_on_cpu = test_on_cpu)
    return distiller.train()


def compute_num_params(seg_model, clf_model):
    return count_params(seg_model.trainable_weights) + count_params(clf_model.trainable_weights)


def main():
    args = cli().parse_args()
    set_loader_params(
        height = args.height,
        width = args.width,
        dil_ksize = args.dil_ksize,
        mixed_sup_N = args.mixed_sup_N,
        dist_trans_w = args.dist_trans_w,
        dist_trans_p = args.dist_trans_p,
        shuffle_buf_size = args.shuffle_buf_size,
        batch_size = args.batch_size
    )
    train_pos, train_neg_iter, test = load_ksdd2(
        args.base_path, args.dataset_json_path, args.train_percentage
    )

    teacher_model_path = args.teacher_model_path
    seg_model = tf.keras.models.load_model(os.path.join(teacher_model_path, 'seg_model.h5'))
    clf_model = tf.keras.models.load_model(os.path.join(teacher_model_path, 'clf_model.h5'))
    teacher = MsModel(seg_model, clf_model)

    if args.networks_config_path != '':
        print(1)
        with open(args.networks_config_path) as f:
            networks_config = yaml.load(f, Loader=yaml.FullLoader)
        stud_seg_model, stud_clf_model = create_models_from_config(networks_config, input_channels=args.input_channels)
    else:
        print(2)
        stud_seg_model, stud_clf_model = create_models(input_channels=args.input_channels)
    student = MsModel(stud_seg_model, stud_clf_model)

    stud_seg_model.summary()
    stud_clf_model.summary()

    epochs = args.epochs
    delta = args.delta
    log_interval = args.log_interval
    lr = args.learning_rate
    test_on_cpu = args.test_on_cpu

    if compute_num_params(student.seg_model, student.clf_model) < compute_num_params(teacher.seg_model, teacher.clf_model) // 2:
        metrics = distill(teacher, student, epochs, delta, log_interval, train_pos, train_neg_iter, test, lr, test_on_cpu)
    else:
        metrics = {"AP_clf": 0.0, "AP_seg": 0.0, "avg_infer_time": 2.0}

    stud_seg_model.save(os.path.join(args.output_path, 'seg_model.h5'))
    stud_clf_model.save(os.path.join(args.output_path, 'clf_model.h5'))

    if args.output_path is not None:
        with open(os.path.join(args.output_path, 'metrics.json'), 'w') as f:
            json.dump(metrics, f, indent=2)


if __name__ == '__main__':
    main()